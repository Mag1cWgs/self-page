## 最优化计算方法

- 开设于本科三年级上学期，作为全方向的选修课程。
- 使用课本：
    - [《最优化计算方法》 （刘浩洋, 户将, 李勇锋，文再文 高等教育出版社）](http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/opt1-short.pdf)
        - [主页](/ProjectDocs/Mathematic/OptFile/最优化：建模、算法与理论_最优化计算方法.html)
        - [勘误](http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/errata-short.pdf)
    - 详细版：[《最优化：建模、算法与理论》 （刘浩洋, 户将, 李勇锋，文再文 高等教育出版社）](http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/opt1.pdf)
        - [刊误](http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/errata.pdf)



---



## 考试相关

### 2024考试题型：
1. 填空题10分（每小题2分，5个小题）
2. 判断题10分（每小题2分，5个小题）
3. 计算题7到8个题（教材中例题，写过的练习题等）

### 最优化方法考察知识点
1. 第一章 最优化简介
    - 最优化问题的一般形式 `3页`
    - 全局和局部最优解 `16页`
    - 优化算法常用的收敛准则，收敛速度 `17—20页`

2. 第二章 基础知识
    - 梯度海瑟矩阵 `28-29页`
    - 凸集仿射集 `36页`
    - 半正定矩阵 `39页`
    - 凸函数： `42页`
        - 例题 2.5  `45页`
    - 次梯度次微分： `51页`
        - 例题 2.10 — 2.12 `55页`

3. 第三章 典型优化问题
    - 线性规划： `62-63页`
        - 习题 3.1 3.2 `78页`
    - 半定规划： `70—72页`
        - 习题 3.9 79页

4. 第四章最优性理论
    - 下降方向： `85页`
        - 线性化可行方向
        - LICQ
        - Slater条件等
    - 无约束可微问题的一阶必要条件，二阶最优性条件： `85—87页 `
        - 例题  `87页`
        - 习题 4.5 `115页`
    - 无约束不可微问题的最优性理论： `89——91页`
    - 对偶理论 例题 `96-100页`
        - 习题 4.7 `115页`
        - 习题 4.13 `116页 ` 
    - 一般约束优化问题的最优性理论：
        - 例题 `107页`
        - 习题 4.10 `116页`
    - 凸优化问题最优性理论：
        - 例题 `111页`
        - 习题 4.11(a)，4.6(b) `116页`

5. 第五章 无约束优化算法
    - 线搜索准则：
        - Armijo 准则 `121页`
        - Wolfe 准则 `123页`
    - 梯度法迭代计算 `128页`
        - 定理 5.2
        - 习题 5.2 `179页`
    - 步长：
        - 精确线性搜索；
        - BB 梯度法
        - BB 梯度法的两种迭代格式推理 `132页`
    - 牛顿类算法：
        - 牛顿方程 牛顿方向 经典牛顿法迭代格式 `141页`
    - 牛顿法的具体迭代计算：
        - 搜索方向 ：
            - 步长：1（即经典牛顿法）
            - 精确线性搜索
    - 拟牛顿类算法：
        - 割线方程 `148页`
        - 拟牛顿矩阵更新公式推理；
            - SR1公式 5.5.9 `150页`
            - BFGS公式推理5.5.12 `151页`

6. 第六章约束优化算法
    - 罚函数法：
        - 二次罚函数法
        - 对数罚函数法
    - 等式约束优化问题
        - 增广拉格朗日函数法 



---



## 第一章 最优化简介

### 考察内容
- 最优化问题的一般形式 `3页`
- 全局和局部最优解 `16页`
- 优化算法常用的收敛准则，收敛速度 `17—20页`

### 1.1 最优化问题概括

#### 1.1.1 最优化问题简介

最优化问题一般可以描述为:
$$
\min \space f(x), \tag{1.1.1} \\
s.t.\space x \in \chi
$$
其中:
- $x=(x_{1}, x_{2}, \cdots, x_{n})^{T}\in \mathbb{R}^{n}$ 是决策变量.
- $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ 是目标函数.
- $\chi \subseteq \mathbb{R}^{n}$ 是约束集合或可行域.
    - 可行域包含的点称为可行解或可行点.
- 记号 `s.t.` 是 “subject to”的缩写，专指约束条件.
- 当 $ \chi = R^{n} $ 时，问题（1.1.1）称为无约束优化问题.
- 集合 $\chi$ 通常可以由约束函数 $ c_{i}(x): \mathbb{R}^{n} \rightarrow \mathbb{R}, i=1, 2, \cdots, m+l $表达成如下形式：
    $$
    \begin{align}
    \chi = \{ x \in \mathbb{R}^{n} \mid  c_{i}(x) & \leq 0, i = 1, 2, \cdots, m, \notag \\
                c_{i}(x) & = 0, i = m+1, m+2, \cdots, m+l \}. \notag
    \end{align}
    $$
- 在所有满足上述约束条件的决策变量中，
    - 使目标函数 $f(x)$ 取最小值的变量 $x^*$ 称为优化问题（1.1.1）的最优解，即对于任意 $x \in X$ 都有 $f(x) \geq f(x^*)$.
    - 如果我们求解在约束集合 $X$ 上目标函数 $f(x)$ 的最大值，则问题（1.1.1）中的 “ $ \min $ ” 应相应地替换为 “ $ \max $ ” .
- 注意到在集合 $X$ 上，函数 $f$ 的最小（最大）值不一定存在，但是其下（上）确界 “ $\inf f(\sup f)$ ” 总是存在的.
    - 当目标函数的最小（最大）值不存在时，我们便关心其下（上）确界，即将问题（1.1.1）中的 “ $ \min (\max) $ ” 改为 “ $ \inf (\sup) $ ” .
- 为了叙述简便，问题（1.1.1）中的 $x$ 为 $\mathbb{R}^n$ 空间中的向量.
- 实际上，根据具体应用和需求，$x$ 还可以是矩阵、多维数组或张量等，本书介绍的很多理论和算法可以相应推广.


### 1.2 实例：稀疏优化（略）
### 1.3 实例：深度学习（略）
### 1.4 最优化的基本概念

#### 1.4.6  全局和局部最优解
在求解最优化问题之前，先介绍最小化问题(1.1.1)的**最优解**的定义．  

**定义1.1**（最优解）对于可行点 $\bar{x}$（即 $\bar{x} \in X$），定义如下概念：
1. 如果 $f(\bar{x}) \le f(x), \forall x \in X$，那么称 $\bar{x}$ 为问题（1.1.1）的 **全局极小解（点）**，有时也称为（全局）最优解或最小值点；
2. 如果存在 $\bar{x}$ 的一个 $\varepsilon$ 邻域 $N_{\varepsilon}(\bar{x})$ 使得 $f(\bar{x}) \le f(x), \forall x \in N_{\varepsilon}(\bar{x}) \cap X$，那么称 $\bar{x}$ 为问题（1.1.1）的 **局部极小解（点）**，有时也称为局部最优解；
3. 进一步地，如果有 $f(\bar{x}) < f(x), \forall x \in N_{\varepsilon}(\bar{x}) \cap X, x \neq \bar{x}$ 成立，则称 $\bar{x}$ 为问题（1.1.1）的 **严格局部极小解（点）**。
4. 如果一个点是局部极小解，但不是严格局部极小解，我们称之为**非严格局部极小解（点）**．

#### 1.4.7 优化算法

1. **迭代算法产生原因**
    - 在给定优化问题之后，我们要考虑如何求解．根据优化问题的不同形式，其求解的困难程度可能会有很大差别．
    - 对于一个优化问题，如果我们能用代数表达式给出其最优解，那么这个解称为**显式解**，对应的问题往往比较简单．
        - 例如二次函数在有界区间上的极小化问题，我们可以通过比较其在对称轴上和区间两个端点处的值得到最优解，这个解可以显式地写出．
    - 但实际问题往往是没有办法显式求解的，因此常采用**迭代算法**．

2. **迭代算法的基本思想**
    - 从一个初始点 $x^0$ 出发，按照某种给定的规则进行迭代，得到一个序列 $\{x^k\}$：
    - 如果迭代在有限步内终止，那么希望最后一个点就是优化问题的解.
    - 如果迭代点列是无穷集合，那么希望该序列的极限点（或者聚点）则为优化问题的解．

3. **收敛准则**
    - 为了使算法能在有限步内终止，我们一般会通过一些收敛准则来保证迭代停在问题的一定精度逼近解上．
    - 对于无约束优化问题，常用的收敛准则有
        $$
        \frac{f(x^{k}) - f^*}{\max \{ |f^*|, 1 \}} \leq \varepsilon_{1},
        \quad ||
        \nabla f(x^{k})|| \leq \varepsilon_{2} \tag{1.4.1}
        $$
        - 其中 $\varepsilon _{1}, \varepsilon _{2}$ 为给定的很小的正数.
        - $||\cdot ||$ 表示某种范数
            - 这里可以简单理解为 $l_{2}$ 范数：$\displaystyle||x||_{2}=(\sum _{i=1}^{n}x_{i}^{2})^{1/2}$，第二章将会给出范数的一般定义）.
        - $f^*$ 为函数 $f$ 的最小值（假设已知或者以某种方式估计得到）.
        - $\nabla f(x^{k})$ 表示函数 $f$ 在点 $x$ 处的梯度（光滑函数在局部最优点处梯度为零向量，第四章中会给出更多介绍）.
    - 对于约束优化问题，还需要考虑约束违反度。具体地，要求最后得到的点满足
        $$
        \begin{align}
          c_{i}(x^{k})  & \le \varepsilon _{3}, i=1, 2, \cdots, m, \notag   \\
        | c_{i}(x^{k})| & \le \varepsilon _{4}, i=m+1, m+2, \cdots, m+l, \notag
        \end{align}
        $$
        - 其中 $\varepsilon _{3}, \varepsilon _{4}$ 为很小的正数，用来刻画 $x^{k}$ 的可行性.

4. **解的最优性**
    - 除了约束违反度之外，我们也要考虑 $x^{k}$ 与最优解之间的距离.
        - 如 (1.4.1)式中给出的函数值与最优值的相对误差.
    - 由于一般情况下事先并不知道最优解，在**最优解唯一**的情形下一般使用某种**基准算法**来得到 $x^*$ 的一个估计，之后计算其与 $x^{k}$ 的距离以评价算法的性能.
    - 因为约束的存在，我们不能简单地用目标函数的梯度来判断最优性.
        - 实际中采用的**判别准则**是**点的最优性条件**的**违反度**（关于约束优化的最优性条件，会在第四章中给出）.

5. **停机准则**
    - 对于一个具体的算法，根据其设计的出发点，我们不一定能得到一个高精度的逼近解.
    - 此时，为了避免无用的计算开销，我们还需要一些停机准则来及时停止算法的进行。常用的停机准则有：
    $$
    \frac{||x^{k+1}-x^{k}||}{\max \{ ||x^{k}||, 1\}}\le \varepsilon _{5},
     \space 
    \frac{|f(x^{k+1})-f(x^{k})|}{\max \{ |f(x^{k})|, 1\}}\le \varepsilon _{6},
    $$
    - 这里的各个 $\varepsilon$ 一般互不相等.
    - 上面的准则分别表示相邻迭代点和其对应目标函数值的**相对误差很小**.
    - 在算法设计中，这两个条件往往只能反映迭代点列**接近收敛**，但**不能**代表收敛到优化问题的最优解.

6. **依点列收敛**
    1. 在算法设计中，一个重要的标准是算法产生的点列是否收敛到优化问题的解．
    2. 对于问题(1.1.1)，其可能有很多局部极小解和全局极小解，但所有全局极小解对应的目标函数值，即优化问题的最小值 $f^*$ 是一样的．
    3. 考虑**无约束**的情形，对于一个算法：  
        给定初始点$x^{0}$，记其迭代产生的点列为 $\{x^{k}\}$，如果 $\{x^{k}\}$ 在某种范数 $||\cdot ||$ 的意义下满足：
        $$
        \lim _{k\rightarrow \infty}||x^{k}-x^{*}||=0
        $$
        且收敛的点 $x^*$ 为一个局部（全局）极小解，那么我们称该点列收敛到局部（全局）极小解，
        相应的算法称为是**依点列收敛到局部（全局）极小解**的.
    4. 对于**带约束**的情形：  
        给定初始点 $x^{0}$，算法产生的点列 $\{x^{k}\}$ 不一定是可行的（即 $x^{k}\in \chi$ 未必对任意 $k$ 成立）.
        - 考虑到约束违反的情形，我们需要保证 $\{x^{k}\}$ 在收敛到 $x^*$ 的时候，其违反度是可接受的.
        - 除此要求之外，算法的收敛性的定义和无约束情形相同.
    
7. **依函数值收敛**
    1. 在算法的收敛分析中，初始迭代点 $x^{0}$ 的选取也尤为重要。
        - 比如一般的牛顿法，只有在初始点足够接近局部（全局）最优解时，才能收敛。
        - 但是这样的初始点的选取往往比较困难，此时我们更想要的是一个从任何初始点出发都能收敛的算法。
    2. 因此优化算法的研究包括如何设计全局化策略
        - 将已有的可能发散的优化算法修改得到一个新的全局收敛到局部（全局）最优解的算法。
        - 比如通过采用合适的全局化策略，我们可以修正一般的牛顿法使得修改后的算法是全局收敛到局部（全局）最优解的。
    3. 进一步地，如果从**任意**初始点 $x^{0}$ 出发，算法都是依点列收敛到局部（全局）极小解的，我们称该算法是**全局依点列收敛到局部（全局）极小解**的。
        - 相应地，如果记对应的函数值序列为 $\{f(x^{k})\}$，我们还可以定义算法的 **（全局）依函数值收敛到局部（全局）极小值** 的概念。
    4. 对于**凸优化**问题，因为其**任何局部最优解都为全局最优解**，算法的收敛性都是相对于其全局极小而言的。
        - 除了点列和函数值的收敛外，实际中常用的还有每个迭代点的最优性条件（如无约束优化问题中的梯度范数，约束优化问题中的最优性条件违反度等等）的收敛。 

8. **收敛速度**
    1. **Q-收敛速度**  
        对于同一个优化问题，其求解算法可以有很多.  
        在设计和比较不同的算法时，另一个重要的指标是算法的渐进收敛速度.  
        我们以点列的 **Q-收敛速度** （*Q* 的含义为 “quotient” ）为例:  
        （函数值的 Q-收敛速度 可以类似地定义）   
        **设** $\{x^{k}\}$ 为算法产生的迭代点列且收敛于 $x^*$，
        - 若对充分大的 $k$ 有
        $$
        \frac{||x^{k+1}-x^{*}||}{||x^{k}-x^{*}||}\le a, \space a\in (0, 1),
        $$
        则称算法（点列）是 **Q-线性收敛的**；
        - 若满足
        $$
        \lim _{k\rightarrow \infty}\frac{||x^{k+1}-x^{*}||}{||x^{k}-x^{*}||}=0,
        $$
        称算法（点列）是 **Q-超线性收敛的**；
        - 若满足
        $$
        \lim _{k\rightarrow \infty}\frac{||x^{k+1}-x^{*}||}{||x^{k}-x^{*}||}=1,
        $$
        称算法（点列）是**Q-次线性收敛的**；
        - 若对充分大的 $k$ 满足
        $$
        \frac{||x^{k+1}-x^{*}||}{||x^{k}-x^{*}||^{2}}\le a, \space a>0,
        $$
        则称算法（点列）是 **Q-二次收敛** 的.
        - 类似地，也可定义更一般的 Q-r 次收敛 $(r>1)$.

    2. **R-收敛速度**  
        除 Q-收敛速度外，另一常用概念是 **R-收敛速度**（R 的含义为“root”）.  
        以点列为例：  
        **设** $\{x^{k}\}$ 为算法产生的迭代点且收敛于 $x^*$，  
        - 若存在 Q-线性收敛 于 0 的非负序列 $t_{k}$ 并且对任意的 $k$ 有
        $$
        ||x^{k}-x^{*}||\le t_{k}
        $$
        成立，则称算法（点列）是 **R-线性收敛** 的.
        - 类似地，可定义 **R-超线性收敛** 和 **R-二次收敛** 等收敛速度.
        - 从 R-收敛速度的定义可以看出序列 $\{||x^{k}-x^{*}||\}$ 被另一趋于 0 的序列 $\{t_{k}\}$ 控制.
        - 当知道 $t_{k}$ 的形式时，我们也称算法（点列）的收敛速度为 $\mathcal{O}(t_{k})$.


9. **复杂度**
    - 与收敛速度密切相关的概念是优化算法的 **复杂度** $N(\varepsilon)$
        - 即计算出给定精度 $\varepsilon$ 的解所需的迭代次数或浮点运算次数.
    - 在实际应用中，这两种定义复杂度的方式均很常见.
    - 如果能较准确地估计每次迭代的运算量，则可以由算法所需迭代次数推出所需浮点运算次数.
    - 我们用具体的例子来进一步解释算法复杂度：  
        **设**某一算法产生的迭代序列 $\{x^{k}\}$ 满足:
        $$
        f(x^{k})-f(x^{*})\le \frac{c}{\sqrt{k}}, \space \forall k>0,
        $$
        - 其中 $c>0$ 为常数，$x^*$ 为全局极小点.
        - 如果需要计算算法满足精度 $f(x^{k})-f(x^*)\le \varepsilon$ 所需的迭代次数:
            - 只需令 $\frac{c}{\sqrt{k}}\le \varepsilon$
            - 则得到 $k\ge \frac{c^{2}}{\varepsilon ^{2}}$
            - 因此该优化算法对应的（迭代次数）复杂度为 $N(\varepsilon)=\mathcal{O}(\frac{1}{\varepsilon ^{2}})$.
    - 注意：
        - 渐进收敛速度更多的是考虑迭代次数充分大的情形.
        - 而复杂度给出了算法迭代有限步之后产生的解与最优解之间的定量关系.



---



## 第二章 基础知识

### 考察内容
- 梯度海瑟矩阵 `28-29页`
- 凸集仿射集 `36页`
- 半正定矩阵 `39页`
- 凸函数： `42页`
    - 例题 2.5  `45页`
- 次梯度次微分： `51页`
    - 例题 2.10 — 2.12 `55页`

###  2.1 范数
#### 2.1.1 向量范数
1. **向量范数**  

    **定义2.1**（范数）称一个从向量空间 $\mathbb{R}^{n}$ 到实数域 $\mathbb{R}$ 的非负函数 $||\cdot ||$ 为 **范数**，如果它满足：

    * (1) 正定性：对于所有的 $v \in \mathbb{R}^{n}$，有 $||v|| \ge 0$，且 $||v|| = 0$ 当且仅当 $v = 0$；
    * (2) 齐次性：对于所有的 $v \in \mathbb{R}^{n}$ 和 $\alpha \in \mathbb{R}$，有 $||\alpha v|| = |\alpha| \cdot ||v||$；
    * (3) 三角不等式：对于所有的 $v, w \in \mathbb{R}^{n}$，有 $||v + w|| \le ||v|| + ||w||$.

2. 最常用的向量范数为 $l_{p}$ 范数($p \ge 1$):
    $$
    ||v||_{p} = (|v_{1}|^{p} + |v_{2}|^{p} + \cdots + |v_{n}|^{p})^{\frac{1}{p}};
    $$
    - 当 $p = \infty$ 时，$l_{\infty}$ 范数定义为
        $$
        ||v||_{\infty} = \max_i |v_{i}|.
        $$
    - 其中 $p=1, 2, \infty$ 的情形最重要，分别记为 $||\cdot ||_{1}$, $||\cdot ||_{2}$ 和 $||\cdot ||_{\infty}$.   
    - 在不引起歧义的情况下，我们有时省略 $l_{2}$ 范数的角标，记为 $||\cdot ||$.   
3. 正定矩阵**诱导范数**  
    在最优化问题算法构造和分析中，也常常遇到由正定矩阵 $A$ 诱导的范数，
    即 $||x||_{A}\xlongequal{{def}}\sqrt{x^{T}Ax}$.   
    - 根据正定矩阵的定义，很容易验证 $||\cdot ||_{A}$ 定义了一个范数.

4. 柯西不等式  
    对向量的 $l_{2}$ 范数，我们有常用的柯西（Cauchy）不等式：

    **命题2.1**（柯西不等式）设 $a, b \in \mathbb{R}^{n}$，则
    $$
    |a^{T}b|\le ||a||_{2}||b||_{2},
    $$
    等号成立当且仅当 $a$ 与 $b$ 线性相关.

#### 2.1.2 矩阵范数

和向量范数类似，矩阵范数是定义在矩阵空间上的非负函数，并且满足正定性、齐次性和三角不等式。
1. **矩阵范数**  
    向量的 $l_{p}$ 范数可以比较容易地推广到矩阵的 $l_{p}$ 范数，本书常用 $p=1,2$ 的情形。
    - 当 $p=1$ 时，矩阵 $A \in \mathbb{R}^{m \times n}$ 的 $l_{1}$ 范数定义为
        $$
        ||A||_{1} = \sum _{i=1}^{m}\sum _{j=1}^{n}|a_{ij}|,
        $$
        即 $||A||_{1}$ 为 $A$ 中所有元素绝对值的和。
    - 当 $p=2$ 时，此时得到的是矩阵的 Frobenius 范数（下称 $F$ 范数），记为 $||A||_{F}$。它可以看成是向量的 $l_{2}$ 范数的推广，即所有元素平方和开根号：
        $$
        ||A||_{F} = \sqrt{Tr(AA^{T})} = \sqrt{\sum _{ij} a_{ij}^{2}} (2.1.1)
        $$
        - $Tr(X)$ 表示方阵 $X$ 的迹。
        - 矩阵的 $F$ 范数具有正交不变性:
            即对于任意的正交矩阵 $U \in \mathbb{R}^{m \times m}, V \in \mathbb{R}^{n \times n}$，我们有
            $$
            \begin{matrix}
            \begin{align}
            ||UAV||_{F}^{2} &= Tr(UAVV^{T}A^{T}U^{T}) &= Tr(UAA^{T}U^{T}) \\
            &= Tr(AA^{T}U^{T}U) &= Tr(AA^{T}) = ||A||_{F}^{2}
            \end{align}
            \end{matrix}
            $$
            其中第三个等号成立是因为 $Tr(AB) = Tr(BA)$。

2. **算子范数**
    - 给定矩阵 $A \in \mathbb{R}^{m \times n}$，以及 $m$ 维和 $n$ 维空间的向量范数 $||\cdot ||_{(m)}$ 和 $||\cdot ||_{(n)}$，其诱导的矩阵范数定义如下：
        $$
        ||A||_{(m, n)} = \max _{x \in R^{n}, ||x||_{(n)}=1} ||Ax||_{(m)},
        $$
        - 容易验证 $||\cdot ||_{(m, n)}$ 满足范数的定义。
    - 如果将 $||\cdot ||_{(m)}$ 和 $||\cdot ||_{(n)}$ 都取为相应向量空间的 $l_p$ 范数，我们可以得到矩阵的 $p$ 范数。
        - 本书经常用到的是矩阵的 2 范数，即
            $$
            ||A||_{2} = \max _{x \in R^{n}, ||x||_{2}=1} ||Ax||_{2}.
            $$
        容易验证（见习题 2.1），矩阵的 2 范数是该矩阵的最大奇异值。
3. 算子范数的**相容性**  
    根据算子范数的定义，所有算子范数都满足如下性质：
    $$
    ||Ax||_{(m)} \le ||A||_{(m, n)} ||x||_{(n)}. (2.1.2)
    $$
    - 例如  
        当 $m=n=2$ 时，$||Ax||_{2} \le ||A||_{2} ||x||_{2}$。
    - 性质 (2.1.2) 又被称为矩阵范数的 **相容性**  
    即 $||\cdot ||_{(m, n)}$ 与 $||\cdot ||_{(m)}$ 和 $||\cdot ||_{(n)}$ 是相容的。
    - 并非所有矩阵范数都与给定的向量范数相容，在今后的应用中读者需要注意这一问题。

    > [!tip|label:注2-1]
    > 和矩阵 2 范数类似，向量的 $l_1$ 范数以及 $l_{\infty}$ 范数均可诱导出相应的矩阵范数（分别为矩阵的 1 范数和无穷范数）  
    > 在多数数值代数教材中将它们记为 $||\cdot ||_{1}$ 和 $||\cdot ||_{\infty}$。  
    > 然而本书较少涉及这两个范数，因此我们将 $||A||_{1}$ 定义为矩阵 $A$ 中所有元素绝对值的和。  
    > 读者应当注意它和其他数值代数教材中定义的不同。

4. 除了矩阵 2 范数以外，另一个常用的矩阵范数为 **核范数**。
    - 给定矩阵 $A \in \mathbb{R}^{m \times n}$，其核范数定义为
        $$
        ||A||_{*} = \sum _{i=1}^{r} \sigma _{i},
        $$
        - $\sigma _{i}, i=1, 2, \cdots, r$ 为 $A$ 的所有非零奇异值
        - $r = \text{rank}(A)$
    - 类似于向量的 $l_1$ 范数的保稀疏性，我们也经常通过限制矩阵的核范数来保证矩阵的低秩性。
    - 同时，根据范数的三角不等式（下文中的凸性），相应的优化问题可以有效求解。

#### 2.1.3 矩阵内积

1. **Frobenius 内积**  
    - 对于矩阵空间 $\mathbb{R}^{n \times n}$ 的两个矩阵 $A$ 和 $B$，除了定义它们各自的范数以外，我们还可以定义它们之间的内积。
    - 范数一般用来衡量矩阵的模的大小，而内积一般用来表征两个矩阵（或其张成的空间）之间的夹角。
    - 这里，我们介绍一种常用的内积——Frobenius 内积。$m \times n$ 矩阵 $A$ 和 $B$ 的 Frobenius 内积定义为
        $$
        \langle A, B \rangle = \text{Tr}(AB^T) = \sum_{i=1}^m \sum_{j=1}^n a_{ij} b_{ij}.
        $$
        易知其为两个矩阵逐分量相乘的和，因而满足内积的定义。
    - 当 $A = B$ 时，$\langle A, B \rangle$ 等于矩阵 $A$ 的 $F$ 范数的平方。

2. **柯西不等式**  
    和向量范数相似，我们也有矩阵范数对应的柯西不等式：

    **命题 2.2**（矩阵范数的柯西不等式）设 $A, B \in \mathbb{R}^{m \times n}$，则
    $$
    |\langle A, B \rangle| \le ||A||_{F} ||B||_{F},
    $$
    等号成立当且仅当 $A$ 和 $B$ 线性相关。


### 2.2 导数

#### 2.2.1 梯度与海瑟矩阵
1. **梯度的定义**  

    **定义 2.2**（梯度）给定函数 $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$，且 $f$ 在点 $x$ 的一个邻域内有意义，若存在向量 $g \in \mathbb{R}^{n}$ 满足
    $$
    \lim _{p \to 0} \frac{f(x+p) - f(x) - g^{T} p}{||p||} = 0, 
    \tag{2.2.1}
    $$
    - 其中 $||\cdot ||$ 是任意的向量范数
    - 就称 $f$ 在点 $x$ 处 **可微**（或 Fr\'echet 可微）。
        - 此时 $g$ 称为 $f$ 在点 $x$ 处的 **梯度**，记作 $\nabla f(x)$。
    - 如果对区域 $D$ 上的每一个点 $x$ 都有 $\nabla f(x)$ 存在，则称 $f$ 在 $D$ 上可微。

2. **梯度的算子记号**  
    - 若 $f$ 在点 $x$ 处的梯度存在，在 (2.2.1) 式中令 $p = \varepsilon e_{i}$，$e_{i}$ 是第 $i$ 个分量为 1 的单位向量，可知 $\nabla f(x)$ 的第 $i$ 个分量为 $\frac{\partial f(x)}{\partial x_{i}}$。
    - 即
        $$
        \nabla f(x) = \left[ \frac{\partial f(x)}{\partial x_{1}}, \frac{\partial f(x)}{\partial x_{2}}, \cdots, \frac{\partial f(x)}{\partial x_{n}} \right]^{T}.
        $$

    - 如果只关心对一部分变量的梯度，可以通过对 $\nabla$ 加下标来表示。
        - 例如， $\nabla _{x}f(x, y)$ 表示将 $y$ 视为常数时 $f$ 关于 $x$ 的梯度。

3. **海瑟矩阵的定义**  
    - 对应于一元函数的二阶导数，对于多元函数我们可以定义其海瑟矩阵。

    **定义 2.3**（海瑟矩阵） 如果函数 $f(x): \mathbb{R}^{n} \rightarrow \mathbb{R}$ 在点 $x$ 处的二阶偏导数 $\frac{\partial ^2 f(x)}{\partial x_{i} \partial x_{j}}$，$i, j = 1, 2, \cdots, n$ 都存在，则
    $$
    \nabla ^2 f(x) = 
    \begin{bmatrix}
    \frac{\partial ^2 f(x)}{\partial x_1^2} & \frac{\partial ^2 f(x)}{\partial x_1 \partial x_2} & \frac{\partial ^2 f(x)}{\partial x_1 \partial x_3} & \cdots & \frac{\partial ^2 f(x)}{\partial x_1 \partial x_n} \\
    \frac{\partial ^2 f(x)}{\partial x_2 \partial x_1} & \frac{\partial ^2 f(x)}{\partial x_2^2} & \frac{\partial ^2 f(x)}{\partial x_2 \partial x_3} & \cdots & \frac{\partial ^2 f(x)}{\partial x_2 \partial x_n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial ^2 f(x)}{\partial x_n \partial x_1} & \frac{\partial ^2 f(x)}{\partial x_n \partial x_2} & \frac{\partial ^2 f(x)}{\partial x_n \partial x_3} & \cdots & \frac{\partial ^2 f(x)}{\partial x_n^2}
    \end{bmatrix}
    $$
    称为 $f$ 在点 $x$ 处的海瑟矩阵。

    - 当 $\nabla ^{2} f(x)$ 在区域 $D$ 上的每个点 $x$ 处都存在时，称 $f$ 在 $D$ 上二阶可微。
    - 若 $\nabla ^{2} f(x)$ 在 $D$ 上还连续，则称 $f$ 在 $D$ 上二阶连续可微，可以证明此时海瑟矩阵是一个对称矩阵。

4. **雅可比矩阵**  
    当 $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ 是向量值函数时，我们可以定义它的雅可比（Jacobi）矩阵 $J(x) \in \mathbb{R}^{m \times n}$，它的第 $i$ 行是分量 $f_{i}(x)$ 梯度的转置，即
    $$
    J(x) = \begin{bmatrix}
    \frac{\partial f_{1}(x)}{\partial x_{1}} & \frac{\partial f_{1}(x)}{\partial x_{2}} & \cdots & \frac{\partial f_{1}(x)}{\partial x_{n}} \\
    \frac{\partial f_{2}(x)}{\partial x_{1}} & \frac{\partial f_{2}(x)}{\partial x_{2}} & \cdots & \frac{\partial f_{2}(x)}{\partial x_{n}} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial f_{m}(x)}{\partial x_{1}} & \frac{\partial f_{m}(x)}{\partial x_{2}} & \cdots & \frac{\partial f_{m}(x)}{\partial x_{n}}
    \end{bmatrix}
    $$
    此外容易看出，梯度 $\nabla f(x)$ 的雅可比矩阵就是 $f(x)$ 的海瑟矩阵。

5. 泰勒展开

6. 梯度利普希茨(Lipschitz) 连续的函数

7. 二次上界

#### 2.2.2 矩阵变量函数的导数


### 2.3 广义实值函数(略)
### 2.4 凸集

#### 2.4.1 相关定义

#### 2.4.2 重要的凸集

#### 2.4.3 保凸的运算

#### 2.4.4 分离超平面定理

### 2.5 凸函数

#### 2.5.1 定义

#### 2.5.2 凸函数判定定理

#### 2.5.3 保凸的运算

#### 2.5.4 凸函数性质

### 2.6 共轭函数
### 2.7 次梯度

#### 2.7.1 定义

#### 2.7.2 性质

#### 2.7.3 计算规则



---



## 第三章 典型优化问题

### 考察内容
- 线性规划： `62-63页`
    - 习题 3.1 3.2 `78页`
- 半定规划： `70—72页`
    - 习题 3.9 79页

### 3.1 线性规划
### 3.2 最小二乘问题
### 3.3 复合优化问题
### 3.4 随机优化问题
### 3.5 半定规划
### 3.6 矩阵优化
### 3.7 优化模型语言


---



## 第四章最优性理论

### 考察内容
- 下降方向： `85页`
    - 线性化可行方向
    - LICQ
    - Slater条件等
- 无约束可微问题的一阶必要条件，二阶最优性条件： `85—87页 `
    - 例题  `87页`
    - 习题 4.5 `115页`
- 无约束不可微问题的最优性理论： `89——91页`
- 对偶理论 例题 `96-100页`
    - 习题 4.7 `115页`
    - 习题 4.13 `116页 ` 
- 一般约束优化问题的最优性理论：
    - 例题 `107页`
    - 习题 4.10 `116页`
- 凸优化问题最优性理论：
    - 例题 `111页`
    - 习题 4.11(a)，4.6(b) `116页`


### 4.1 最优化问题解的存在性
### 4.2 无约束可微问题的最优性理论
### 4.3 无约束不可微问题的最优性理论
### 4.4 对偶理论
### 4.5 一般约束优化问题的最优性理论
### 4.6 带约束凸优化问题的最优性理论
### 4.7 约束优化最优性理论应用实例


---



## 第五章 无约束优化算法

### 考察内容
- 线搜索准则：
    - Armijo 准则 `121页`
    - Wolfe 准则 `123页`
- 梯度法迭代计算 `128页`
    - 定理 5.2
    - 习题 5.2 `179页`
- 步长：
    - 精确线性搜索；
    - BB 梯度法
    - BB 梯度法的两种迭代格式推理 `132页`
- 牛顿类算法：
    - 牛顿方程 牛顿方向 经典牛顿法迭代格式 `141页`
- 牛顿法的具体迭代计算：
    - 搜索方向 ：
        - 步长：1（即经典牛顿法）
        - 精确线性搜索
- 拟牛顿类算法：
    - 割线方程 `148页`
    - 拟牛顿矩阵更新公式推理；
        - SR1公式 5.5.9 `150页`
        - BFGS公式推理5.5.12 `151页`

### 5.1 线搜索方法
### 5.2 梯度类算法
### 5.3 次梯度算法
### 5.4 牛顿类算法
### 5.5 拟牛顿类算法
### 5.6 信赖域算法
### 5.7 非线性最小二乘问题算法
### 5.8 总结


---



## 第六章约束优化算法

### 考察内容
- 罚函数法：
    - 二次罚函数法
    - 对数罚函数法
- 等式约束优化问题
    - 增广拉格朗日函数法 


### 6.1 罚函数法
### 6.2 增广拉格朗日函数法




